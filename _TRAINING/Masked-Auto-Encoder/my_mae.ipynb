{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\rapto\\miniconda3\\envs\\tf\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\rapto\\miniconda3\\envs\\tf\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting seeds for reproducibility.\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "\n",
    "# Plotting settings\n",
    "sns.set()\n",
    "plt.rc('font', family = 'serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_testset(dataset_name):\n",
    "    \"\"\"\n",
    "    `DariusAf_Deepfake_Database` (train_test)\n",
    "    `Celeb-avg-30-(train/test)`\n",
    "    `Celeb-rnd-30-(train/test)`\n",
    "    `Celeb-diff-30-(train/test)`\n",
    "    \"\"\"\n",
    "    path_2_root = \"../..\"\n",
    "    if dataset_name == \"DariusAf_Deepfake_Database\":\n",
    "        testset = f\"{path_2_root}/_DATASETS/DariusAf_Deepfake_Database/train_test\"\n",
    "    elif dataset_name == \"Celeb-avg-30-test\":\n",
    "        testset = f\"{path_2_root}/_DATASETS/Celeb-DF-v2/Celeb-avg-30-test\"\n",
    "    elif dataset_name == \"Celeb-rnd-30-test\":\n",
    "        testset = f\"{path_2_root}/_DATASETS/Celeb-DF-v2/Celeb-rnd-30-test\"\n",
    "    elif dataset_name == \"Celeb-diff-30-test\":\n",
    "        testset = f\"{path_2_root}/_DATASETS/Celeb-DF-v2/Celeb-diff-30-test\"\n",
    "\n",
    "    elif dataset_name == \"Celeb-avg-30-train\":\n",
    "        testset = f\"{path_2_root}/_DATASETS/Celeb-DF-v2/Celeb-avg-30\"\n",
    "    elif dataset_name == \"Celeb-rnd-30-train\":\n",
    "        testset = f\"{path_2_root}/_DATASETS/Celeb-DF-v2/Celeb-rnd-30\"\n",
    "    elif dataset_name == \"Celeb-diff-30-train\":\n",
    "        testset = f\"{path_2_root}/_DATASETS/Celeb-DF-v2/Celeb-diff-30\"\n",
    "\n",
    "    # elif dataset_name == \"Celeb-rnd-30-OC-real-train\":\n",
    "    return testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 64\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# PRETRAINING\n",
    "EPOCHS = 100\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 32  # We will resize input images to this size.\n",
    "PATCH_SIZE = IMAGE_SIZE // 10  # Size of the patches to be extracted from the input images.\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "MASK_PROPORTION = 0.75  # We have found 75% masking to give us the best results.\n",
    "\n",
    "# ENCODER and DECODER\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "ENC_PROJECTION_DIM = 128\n",
    "DEC_PROJECTION_DIM = 64\n",
    "ENC_NUM_HEADS = 4\n",
    "ENC_LAYERS = 6\n",
    "DEC_NUM_HEADS = 4\n",
    "DEC_LAYERS = (\n",
    "    2  # The decoder is lightweight but should be reasonably deep for reconstruction.\n",
    ")\n",
    "ENC_TRANSFORMER_UNITS = [\n",
    "    ENC_PROJECTION_DIM * 2,\n",
    "    ENC_PROJECTION_DIM,\n",
    "]  # Size of the transformer layers.\n",
    "DEC_TRANSFORMER_UNITS = [\n",
    "    DEC_PROJECTION_DIM * 2,\n",
    "    DEC_PROJECTION_DIM,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19638 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "DATASET = get_binary_testset(\"Celeb-rnd-30-test\") # NOTE: Using test set bc it's smaller, just for testing!!!\n",
    "IMG_DATAGEN = ImageDataGenerator()#rescale=1./255)\n",
    "# IMGWIDTH = 64\n",
    "GEN = IMG_DATAGEN.flow_from_directory(DATASET,\n",
    "                                      target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      seed=SEED,\n",
    "                                      class_mode=\"binary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Rescaling(1 / 255.0),\n",
    "            layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "            layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "        ],\n",
    "        name=\"train_data_augmentation\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_test_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [layers.Rescaling(1 / 255.0), layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),],\n",
    "        name=\"test_data_augmentation\",\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size=PATCH_SIZE, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Assuming the image has three channels each patch would be\n",
    "        # of size (patch_size, patch_size, 3).\n",
    "        self.resize = layers.Reshape((-1, patch_size * patch_size * 3))\n",
    "\n",
    "    def call(self, images):\n",
    "        # Create patches from the input images\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "\n",
    "        # Reshape the patches to (batch, num_patches, patch_area) and return it.\n",
    "        patches = self.resize(patches)\n",
    "        return patches\n",
    "\n",
    "    def show_patched_image(self, images, patches):\n",
    "        # This is a utility function which accepts a batch of images and its\n",
    "        # corresponding patches and help visualize one image and its patches\n",
    "        # side by side.\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        print(f\"Index selected: {idx}.\")\n",
    "\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(keras.utils.array_to_img(images[idx]))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        n = int(np.sqrt(patches.shape[1]))\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        for i, patch in enumerate(patches[idx]):\n",
    "            ax = plt.subplot(n, n, i + 1)\n",
    "            patch_img = tf.reshape(patch, (self.patch_size, self.patch_size, 3))\n",
    "            plt.imshow(keras.utils.img_to_array(patch_img))\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # Return the index chosen to validate it outside the method.\n",
    "        return idx\n",
    "\n",
    "    # taken from https://stackoverflow.com/a/58082878/10319735\n",
    "    def reconstruct_from_patch(self, patch):\n",
    "        # This utility function takes patches from a *single* image and\n",
    "        # reconstructs it back into the image. This is useful for the train\n",
    "        # monitor callback.\n",
    "        num_patches = patch.shape[0]\n",
    "        n = int(np.sqrt(num_patches))\n",
    "        patch = tf.reshape(patch, (num_patches, self.patch_size, self.patch_size, 3))\n",
    "        rows = tf.split(patch, n, axis=0)\n",
    "        rows = [tf.concat(tf.unstack(x), axis=1) for x in rows]\n",
    "        reconstructed = tf.concat(rows, axis=0)\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(64, 32, 32, 3), (64,)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = GEN.next() # (batch size, image size, image size, 3), (32,)\n",
    "[x.shape for x in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index selected: 1.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOLUlEQVR4nO3dyY4kSRHGcYstl6qsLs0Cg0YgnoEb4sSFOw/HG/EiHDgwmqF7ursq11g4zAUJ/75SOl051qP/75guj4yMTMuQ3MLNmmVZlgCQTvtznwCAMoITSIrgBJIiOIGkCE4gqd4N/vVPv6866BLlBWC/LvwKi8bikJ/DArU7R3V9X9ItjRxbR1d8/c9fPsg5f3jY6uM1Fzk26NOIdVcefNjt5JztZi3Hulbff9Yr/fNfD+XrERExjuXPNl7Oes5llGO7v/29+Dp3TiApghNIiuAEkiI4gaQITiApghNIyqZS6pWXwxuTArhpcsMs5d/2RDSXLpldKsgMuX9ilbpp5lkfb5nkmLvE7jwacf59p2f1vU57dObdmkaP6U8dMasP1+hP3ZqUjpxz9QwAN0FwAkkRnEBSBCeQFMEJJEVwAkm9SipFLSibTRHR2MV3w2UVxLp8Y05EzXnpvT45syzvxpbFJAF05iPasfzhTCYlxkX/tw96WiwmUaFSOuOo50xmrO31tZpnfUFGvYkk5rE8b55M+stdSIE7J5AUwQkkRXACSRGcQFIEJ5DUKz34Xla9Imv41VXxfmZOmpVct1pr30zPc0fs1GKiuR5u/dFeDvvZxPHcpTeDTeUmAf/hxPmb483z9T8Q7pxAUgQnkBTBCSRFcAJJEZxAUgQnkNRNUymvwaZnROrDdmOofDjftki4YZqlsXVs9CFFF4Tqcktzax5ut+ev6wHpA7prrx9u70QLioiI1pyHKj20zPoC13QA4c4JJEVwAkkRnEBSBCeQFMEJJEVwAknZVIpb1vapAzWWpNfBZ8DtpujMmC1VY+riNJNIOZg6O+7r7Ex9IduZQKUp3Gee9Ieezf3nMusL0rSm1YRIBbk6QZOo0eRw5wSSIjiBpAhOICmCE0iK4ASSIjiBpF5Ipbiy/zVvlyOVUr275IZcF3BbqMv8365Mq4ZOpEzayvNoKwuN6Z1EZpeLKZ5lMjoxubyTS2W15RSjj4nri5px5wSSIjiBpAhOICmCE0iK4ASSIjiBpD77Al+OSpm4nie2V8ptW1vrEZPierPayLGt2Wmxmi7l1zuzu8SMLY3e8eF2mCxi90ZjWmW3rb4eXW/uP2ae+67V76oz38tms9LnIXDnBJIiOIGkCE4gKYITSIrgBJL6Ra/WKvbB9+ru1de3hXAW87/ZqH4AEfE4rM2Yfr+VOGQrHvKOiPhw0iuyh+koxwazqjn05Z/kZNo7RKdXod2Kclv5Vffi/P2GClfcqYw7J5AUwQkkRXACSRGcQFIEJ5AUwQkk9X+kUq6viVLfJzk/kx2Immu1uAffTXrjcdAPWH/T61zKeiMemDedoX/Yn+TYuH+WY12nf3brzbb4+jnOco5pGBGz+V0N5jr2pmfEIh6md9+y6fwQ9+J17pxAUgQnkBTBCSRFcAJJEZxAUgQnkJRPpZha9q7zcqg6POZ4NpFidw+4mWLM1NKpyhBFRMy1E9VpmLo4Zpl/WOldKZtBf927uZwWOR0Ocs77g06XfPdRjz2fy/WKIiIu4qv5eqvP/at7/Zm/3KlERcQ3u50c+/pez9uKbNVo2jt8PO3l2B/F69w5gaQITiApghNIiuAEkiI4gaQITiAp39m6Mq+g0xsuNeO6aFdWYmrU0rYrtvQKu2PMZ1PctXf1wmbRoToiomv0LoyHQey0OOnz0MmBiNkUIXua9PX/8VAuDPbhqD/Xv/d6t82vz3o7yGUy38usr9X9unz+p0m/1/fHJzlGKgX4zBCcQFIEJ5AUwQkkRXACSRGcQFI+lWL7hpjuxCpVYet7Vf5PVBTW+jzKjJnra5bs908f5NhlKRfPioh4eLwrvr5Shb8iYm2+s91K7/h4vNNJmH/uy+f/9PRezjmbImQx6+t4OemiYR9bvatmupTnHU0Vr/dHfTyFOyeQFMEJJEVwAkkRnEBSBCeQFMEJJOULfMldHWF7dqsMzLLo4/l273qHwKctq+X5dvVuZkWCpi6LFQexzB8RcZn07o2V6F+yhNnVEfp72Zm+LF/t9M9uFtWz/rHXuzrms/7MG9M7ZrvSY3eDPv/dujyvN7tcns7X3we5cwJJEZxAUgQnkBTBCSRFcAJJ2dXaitI3P1HdGEy5+jA1Z27J1k2qXJG1K9GKae+wmIe5T5N+CPxirn/flVdeJ/O9DK4mVKt/Wl+YlhHDUH4A/+13/5JzLqP+zI+iU/ZLY282+hzf3JXHjqO+Hofj9U3kc0QEgP9BcAJJEZxAUgQnkBTBCSRFcAJJvbC+qx9sruPaIOC/tSY10zYmzWKOuZh5ncib3ZuHwzfmv300Y+4Z8J3YULH69ls5Zxn1w/m/ute1jHbm4fa7Qf/21624jiIdFRGx2+rUjMKdE0iK4ASSIjiBpAhOICmCE0iK4ASS8qmU5fon6X+aV365cTWEXGEcx9U5UmO3LDwUYfMbsqO3vR4uJaXHGlMPqBMn2aq0QUR0vdmxYtI2vfkdDGLnzNakRFpzrTa9Tm8M5rOJRt8REdGLebPpHO7qFSncOYGkCE4gKYITSIrgBJIiOIGkCE4gKd/Z2uxKWVz1L7W0bYpWNbZTtstF6PRAtGbshmS6JCJakVXwiRSTSlmO5r30cr7qYr6YVNUcurCWK5TWTqbQ2KX8na3Mz60zKRF39+nMdXT7sbq2fB0HM2ttUjoKd04gKYITSIrgBJIiOIGkCE4gKYITSOqFbSe2XNTVb9aYZXnX9Vq2yn5p7IZsXxnzFziLiY3ph+Leajbpqtn2NimPDZ0rNKZ/Pp0pdrVM+pij6IjtfjtVncMjojW/HbXzJCKiFWkRd6frTX8Y+T5XzwBwEwQnkBTBCSRFcAJJEZxAUn611q2QuQUytQrmHlK371VbT+eGKlskqKXXxhyvMSuyk1k2dmPKIFZxIyJWrut1r39as3kGfBQ7Aeb5oo+36AfwXW2q1px/112/su02aCwVndu5cwJJEZxAUgQnkBTBCSRFcAJJEZxAUjaV0oouwxG+js0iSuqHaQfgUyI2b2PGbqf2+Xs1bTIpDOdiHiq/zPr6jzIdYdINruWC+TptMwlxIU/jWc4ZJ51mac117Dv983dplkn8vsdZp3ROJ13bSZ7D1TMA3ATBCSRFcAJJEZxAUgQnkBTBCST1QjsGV8fm+rHZPrVfWwsoR5rFvZPfrXDtQMRslvnHVm/5OJmxg6jhtJUzIlZud4y5IONkUg4iLXI+6XSJu779tlyTKCKiF/WKIiL61oSG+NyLaCUREdGM1/++uXMCSRGcQFIEJ5AUwQkkRXACSRGcQFI+lVLZBUGuotvMhhnMsfHkVcyisFlrUimN+U8dB929+mSKbj2LdMSd2Wmx0kNhuknExaQcDufy7o3LaLpQm8/c9joZ1Jt5g0lXXUaR1hnNLp2JAl/ALwbBCSRFcAJJEZxAUgQnkBTBCST1QmdrfBqmS/JS/goaM8e1jpln/X971Bs74t3pVHx9487ddd82qY/RpFJOYvfJ4Wx2pbS6+NfRpKQ2JpWyMjt4ZvEFzCbtpDqYO9w5gaQITiApghNIiuAEkiI4gaSqV2vd2pNsbO3KqNS0f35hrLYq0afmNxCI87fXytQkMqu1p4ue9+5YXvF8MB2e127VeNQrsmczdhQrns+m5cKoHkSPiPcf9UkOjV6Rda0m1Ii707WiRpPDnRNIiuAEkiI4gaQITiApghNIiuAEkvKdrd1SuRmT88xq8rK4/wm95O3m6bHaLtpmlsn2VB3RPEQdk7uQ+nqczSHfiwfO94P+iTyYrtGjSffsTW7pqSuP7Vf6Au9dTaL9Bzk2mhYPszlmK9Is/agvcG/e6y/qfeQMAD8rghNIiuAEkiI4gaQITiApghNIqnpXSmt2g8zyif7avgouT2F2D8ix2/Z3sP+AIuVgd/CYHRPOZdLpgbf78thvHnZyzqnXKa4nk4p4Z9IKb4+H4utLW7czqRt09+rWtbUwNYRGsUNmMt2wN9uNHFO4cwJJEZxAUgQnkBTBCSRFcAJJEZxAUvXtGMxyfiu6As92V0r1maRgUx9mX0qrUim203ddF/DR7HT58VBODzzf38s5+06nG96ZL/t7UUwsIuKHj/vi65u1TonsVms59rjWKYzO7HSZzQ6T51P5HFtTDG17fyfHFO6cQFIEJ5AUwQkkRXACSRGcQFIEJ5AUna0/lcZUzzIFxeamPOZSM7X/qC7bM4ujHk0xsY9nnRJ5eyrvLomIOMx6V8rQl89jt+i0zeOkf8a7Wc9bm10kfafTLOO23BH7aSp3B4+IePf8LMcU7pxAUgQnkBTBCSRFcAJJEZxAUgQnkNRNUymN2K3y05hppe5adtfUunqVLTCVRcPE565Je7w0c3ZjYhfJ2RQFO4z6M7vW8o35PreikNd8OMo5Y6/PY7h/NOchh2K86NTYJM5/MmkndzyFOyeQFMEJJEVwAkkRnEBSBCeQlF+trV7ULK+eteZB4wi9mjWaei6N61ItVocX95/kWlQ7rjN3RTkgv1prmLYFs6khNM3l1dXTqB9SP5vO1r1ZEe9NDaRenP/3H97LOadeP3D+zW9/J8f2Bz3vx6cnOXa5lOct5lvrTH0hhTsnkBTBCSRFcAJJEZxAUgQnkBTBCSRV/eD7YpbK1Zifo9+rNUvvVdkeO6mu1cE0mjpB5g0bUTOnMWmPzrQ6UDV4IiLuTLfmN0O5Ls6013WCnszYatAXa9Nv9dhQPsfhUZ/70OpWDStTQ6hbmW7TX+ihJ1Ef6XLR1yPM70PhzgkkRXACSRGcQFIEJ5AUwQkkRXACSTWLy28A+Nlw5wSSIjiBpAhOICmCE0iK4ASSIjiBpP4D4QbdU3y9pA0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 100 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch of images.\n",
    "image_batch = next(iter(train_ds))\n",
    "\n",
    "# Augment the images.\n",
    "augmentation_model = get_train_augmentation_model()\n",
    "augmented_images = augmentation_model(image_batch)\n",
    "\n",
    "# Define the patch layer.\n",
    "patch_layer = Patches()\n",
    "\n",
    "# Get the patches from the batched images.\n",
    "patches = patch_layer(images=augmented_images)\n",
    "\n",
    "# Now pass the images and the corresponding patches\n",
    "# to the `show_patched_image` method.\n",
    "random_index = patch_layer.show_patched_image(images=augmented_images, patches=patches)\n",
    "\n",
    "# Chose the same chose image and try reconstructing the patches\n",
    "# into the original image.\n",
    "image = patch_layer.reconstruct_from_patch(patches[random_index])\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        projection_dim=ENC_PROJECTION_DIM,\n",
    "        mask_proportion=MASK_PROPORTION,\n",
    "        downstream=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.mask_proportion = mask_proportion\n",
    "        self.downstream = downstream\n",
    "\n",
    "        # This is a trainable mask token initialized randomly from a normal\n",
    "        # distribution.\n",
    "        self.mask_token = tf.Variable(\n",
    "            tf.random.normal([1, patch_size * patch_size * 3]), trainable=True\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, self.num_patches, self.patch_area) = input_shape\n",
    "\n",
    "        # Create the projection layer for the patches.\n",
    "        self.projection = layers.Dense(units=self.projection_dim)\n",
    "\n",
    "        # Create the positional embedding layer.\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=self.projection_dim\n",
    "        )\n",
    "\n",
    "        # Number of patches that will be masked.\n",
    "        self.num_mask = int(self.mask_proportion * self.num_patches)\n",
    "\n",
    "    def call(self, patches):\n",
    "        # Get the positional embeddings.\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        pos_embeddings = self.position_embedding(positions[tf.newaxis, ...])\n",
    "        pos_embeddings = tf.tile(\n",
    "            pos_embeddings, [batch_size, 1, 1]\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        # Embed the patches.\n",
    "        patch_embeddings = (\n",
    "            self.projection(patches) + pos_embeddings\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        if self.downstream:\n",
    "            return patch_embeddings\n",
    "        else:\n",
    "            mask_indices, unmask_indices = self.get_random_indices(batch_size)\n",
    "            # The encoder input is the unmasked patch embeddings. Here we gather\n",
    "            # all the patches that should be unmasked.\n",
    "            unmasked_embeddings = tf.gather(\n",
    "                patch_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, unmask_numbers, projection_dim)\n",
    "\n",
    "            # Get the unmasked and masked position embeddings. We will need them\n",
    "            # for the decoder.\n",
    "            unmasked_positions = tf.gather(\n",
    "                pos_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, unmask_numbers, projection_dim)\n",
    "            masked_positions = tf.gather(\n",
    "                pos_embeddings, mask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, mask_numbers, projection_dim)\n",
    "\n",
    "            # Repeat the mask token number of mask times.\n",
    "            # Mask tokens replace the masks of the image.\n",
    "            mask_tokens = tf.repeat(self.mask_token, repeats=self.num_mask, axis=0)\n",
    "            mask_tokens = tf.repeat(\n",
    "                mask_tokens[tf.newaxis, ...], repeats=batch_size, axis=0\n",
    "            )\n",
    "\n",
    "            # Get the masked embeddings for the tokens.\n",
    "            masked_embeddings = self.projection(mask_tokens) + masked_positions\n",
    "            return (\n",
    "                unmasked_embeddings,  # Input to the encoder.\n",
    "                masked_embeddings,  # First part of input to the decoder.\n",
    "                unmasked_positions,  # Added to the encoder outputs.\n",
    "                mask_indices,  # The indices that were masked.\n",
    "                unmask_indices,  # The indices that were unmaksed.\n",
    "            )\n",
    "\n",
    "    def get_random_indices(self, batch_size):\n",
    "        # Create random indices from a uniform distribution and then split\n",
    "        # it into mask and unmask indices.\n",
    "        rand_indices = tf.argsort(\n",
    "            tf.random.uniform(shape=(batch_size, self.num_patches)), axis=-1\n",
    "        )\n",
    "        mask_indices = rand_indices[:, : self.num_mask]\n",
    "        unmask_indices = rand_indices[:, self.num_mask :]\n",
    "        return mask_indices, unmask_indices\n",
    "\n",
    "    def generate_masked_image(self, patches, unmask_indices):\n",
    "        # Choose a random patch and it corresponding unmask index.\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        patch = patches[idx]\n",
    "        unmask_index = unmask_indices[idx]\n",
    "\n",
    "        # Build a numpy array of same shape as patch.\n",
    "        new_patch = np.zeros_like(patch)\n",
    "\n",
    "        # Iterate of the new_patch and plug the unmasked patches.\n",
    "        count = 0\n",
    "        for i in range(unmask_index.shape[0]):\n",
    "            new_patch[unmask_index[i]] = patch[unmask_index[i]]\n",
    "        return new_patch, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the patch encoder layer.\n",
    "patch_encoder = PatchEncoder()\n",
    "\n",
    "# Get the embeddings and positions.\n",
    "(\n",
    "    unmasked_embeddings,\n",
    "    masked_embeddings,\n",
    "    unmasked_positions,\n",
    "    mask_indices,\n",
    "    unmask_indices,\n",
    ") = patch_encoder(patches=patches)\n",
    "\n",
    "\n",
    "# Show a maksed patch image.\n",
    "new_patch, random_index = patch_encoder.generate_masked_image(patches, unmask_indices)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "img = patch_layer.reconstruct_from_patch(new_patch)\n",
    "plt.imshow(keras.utils.array_to_img(img))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Masked\")\n",
    "plt.subplot(1, 2, 2)\n",
    "img = augmented_images[random_index]\n",
    "plt.imshow(keras.utils.array_to_img(img))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(num_heads=ENC_NUM_HEADS, num_layers=ENC_LAYERS):\n",
    "    inputs = layers.Input((None, ENC_PROJECTION_DIM))\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=ENC_PROJECTION_DIM, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=ENC_TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    outputs = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
    "    return keras.Model(inputs, outputs, name=\"mae_encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(\n",
    "    num_layers=DEC_LAYERS, num_heads=DEC_NUM_HEADS, image_size=IMAGE_SIZE\n",
    "):\n",
    "    inputs = layers.Input((NUM_PATCHES, ENC_PROJECTION_DIM))\n",
    "    x = layers.Dense(DEC_PROJECTION_DIM)(inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=DEC_PROJECTION_DIM, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=DEC_TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    pre_final = layers.Dense(units=image_size * image_size * 3, activation=\"sigmoid\")(x)\n",
    "    outputs = layers.Reshape((image_size, image_size, 3))(pre_final)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"mae_decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoder(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_augmentation_model,\n",
    "        test_augmentation_model,\n",
    "        patch_layer,\n",
    "        patch_encoder,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_augmentation_model = train_augmentation_model\n",
    "        self.test_augmentation_model = test_augmentation_model\n",
    "        self.patch_layer = patch_layer\n",
    "        self.patch_encoder = patch_encoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def calculate_loss(self, images, test=False):\n",
    "        # Augment the input images.\n",
    "        if test:\n",
    "            augmented_images = self.test_augmentation_model(images)\n",
    "        else:\n",
    "            augmented_images = self.train_augmentation_model(images)\n",
    "\n",
    "        # Patch the augmented images.\n",
    "        patches = self.patch_layer(augmented_images)\n",
    "\n",
    "        # Encode the patches.\n",
    "        (\n",
    "            unmasked_embeddings,\n",
    "            masked_embeddings,\n",
    "            unmasked_positions,\n",
    "            mask_indices,\n",
    "            unmask_indices,\n",
    "        ) = self.patch_encoder(patches)\n",
    "\n",
    "        # Pass the unmaksed patche to the encoder.\n",
    "        encoder_outputs = self.encoder(unmasked_embeddings)\n",
    "\n",
    "        # Create the decoder inputs.\n",
    "        encoder_outputs = encoder_outputs + unmasked_positions\n",
    "        decoder_inputs = tf.concat([encoder_outputs, masked_embeddings], axis=1)\n",
    "\n",
    "        # Decode the inputs.\n",
    "        decoder_outputs = self.decoder(decoder_inputs)\n",
    "        decoder_patches = self.patch_layer(decoder_outputs)\n",
    "\n",
    "        loss_patch = tf.gather(patches, mask_indices, axis=1, batch_dims=1)\n",
    "        loss_output = tf.gather(decoder_patches, mask_indices, axis=1, batch_dims=1)\n",
    "\n",
    "        # Compute the total loss.\n",
    "        total_loss = self.compiled_loss(loss_patch, loss_output)\n",
    "\n",
    "        return total_loss, loss_patch, loss_output\n",
    "\n",
    "    def train_step(self, images):\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, loss_patch, loss_output = self.calculate_loss(images)\n",
    "\n",
    "        # Apply gradients.\n",
    "        train_vars = [\n",
    "            self.train_augmentation_model.trainable_variables,\n",
    "            self.patch_layer.trainable_variables,\n",
    "            self.patch_encoder.trainable_variables,\n",
    "            self.encoder.trainable_variables,\n",
    "            self.decoder.trainable_variables,\n",
    "        ]\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        tv_list = []\n",
    "        for (grad, var) in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                tv_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(tv_list)\n",
    "\n",
    "        # Report progress.\n",
    "        self.compiled_metrics.update_state(loss_patch, loss_output)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, images):\n",
    "        total_loss, loss_patch, loss_output = self.calculate_loss(images, test=True)\n",
    "\n",
    "        # Update the trackers.\n",
    "        self.compiled_metrics.update_state(loss_patch, loss_output)\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentation_model = get_train_augmentation_model()\n",
    "test_augmentation_model = get_test_augmentation_model()\n",
    "patch_layer = Patches()\n",
    "patch_encoder = PatchEncoder()\n",
    "encoder = create_encoder()\n",
    "decoder = create_decoder()\n",
    "\n",
    "mae_model = MaskedAutoencoder(\n",
    "    train_augmentation_model=train_augmentation_model,\n",
    "    test_augmentation_model=test_augmentation_model,\n",
    "    patch_layer=patch_layer,\n",
    "    patch_encoder=patch_encoder,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = train_ds # Just for testing the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a batch of test inputs to measure model's progress.\n",
    "test_images = next(iter(test_ds))\n",
    "\n",
    "\n",
    "class TrainMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_interval=None):\n",
    "        self.epoch_interval = epoch_interval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.epoch_interval and epoch % self.epoch_interval == 0:\n",
    "            test_augmented_images = self.model.test_augmentation_model(test_images)\n",
    "            test_patches = self.model.patch_layer(test_augmented_images)\n",
    "            (\n",
    "                test_unmasked_embeddings,\n",
    "                test_masked_embeddings,\n",
    "                test_unmasked_positions,\n",
    "                test_mask_indices,\n",
    "                test_unmask_indices,\n",
    "            ) = self.model.patch_encoder(test_patches)\n",
    "            test_encoder_outputs = self.model.encoder(test_unmasked_embeddings)\n",
    "            test_encoder_outputs = test_encoder_outputs + test_unmasked_positions\n",
    "            test_decoder_inputs = tf.concat(\n",
    "                [test_encoder_outputs, test_masked_embeddings], axis=1\n",
    "            )\n",
    "            test_decoder_outputs = self.model.decoder(test_decoder_inputs)\n",
    "\n",
    "            # Show a maksed patch image.\n",
    "            test_masked_patch, idx = self.model.patch_encoder.generate_masked_image(\n",
    "                test_patches, test_unmask_indices\n",
    "            )\n",
    "            print(f\"\\nIdx chosen: {idx}\")\n",
    "            original_image = test_augmented_images[idx]\n",
    "            masked_image = self.model.patch_layer.reconstruct_from_patch(\n",
    "                test_masked_patch\n",
    "            )\n",
    "            reconstructed_image = test_decoder_outputs[idx]\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "            ax[0].imshow(original_image)\n",
    "            ax[0].set_title(f\"Original: {epoch:03d}\")\n",
    "\n",
    "            ax[1].imshow(masked_image)\n",
    "            ax[1].set_title(f\"Masked: {epoch:03d}\")\n",
    "\n",
    "            ax[2].imshow(reconstructed_image)\n",
    "            ax[2].set_title(f\"Resonstructed: {epoch:03d}\")\n",
    "\n",
    "            plt.show()\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code is taken from:\n",
    "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
    "\n",
    "\n",
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n",
    "\n",
    "len_x_train = GEN.n\n",
    "total_steps = int((len_x_train / BATCH_SIZE) * EPOCHS)\n",
    "warmup_epoch_percentage = 0.15\n",
    "warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "scheduled_lrs = WarmUpCosine(\n",
    "    learning_rate_base=LEARNING_RATE,\n",
    "    total_steps=total_steps,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=warmup_steps,\n",
    ")\n",
    "\n",
    "lrs = [scheduled_lrs(step) for step in range(total_steps)]\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"Step\", fontsize=14)\n",
    "plt.ylabel(\"LR\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Assemble the callbacks.\n",
    "train_callbacks = [TrainMonitor(epoch_interval=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tfa.optimizers.AdamW(learning_rate=scheduled_lrs, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Compile and pretrain the model.\n",
    "mae_model.compile(\n",
    "    optimizer=optimizer, loss=keras.losses.MeanSquaredError(), metrics=[\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\MInf2\\_TRAINING\\Masked-Auto-Encoder\\my_mae.ipynb Cell 21'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MInf2/_TRAINING/Masked-Auto-Encoder/my_mae.ipynb#ch0000027?line=6'>7</a>\u001b[0m val_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MInf2/_TRAINING/Masked-Auto-Encoder/my_mae.ipynb#ch0000027?line=7'>8</a>\u001b[0m val_ds \u001b[39m=\u001b[39m val_ds\u001b[39m.\u001b[39mbatch(BATCH_SIZE)\u001b[39m.\u001b[39mprefetch(AUTO)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MInf2/_TRAINING/Masked-Auto-Encoder/my_mae.ipynb#ch0000027?line=8'>9</a>\u001b[0m mae_model\u001b[39m.\u001b[39;49mtrain_on_batch(X[\u001b[39m0\u001b[39;49m],y[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MInf2/_TRAINING/Masked-Auto-Encoder/my_mae.ipynb#ch0000027?line=9'>10</a>\u001b[0m history \u001b[39m=\u001b[39m mae_model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MInf2/_TRAINING/Masked-Auto-Encoder/my_mae.ipynb#ch0000027?line=10'>11</a>\u001b[0m     train_ds, epochs\u001b[39m=\u001b[39mEPOCHS, validation_data\u001b[39m=\u001b[39mtrain_ds, callbacks\u001b[39m=\u001b[39mtrain_callbacks,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MInf2/_TRAINING/Masked-Auto-Encoder/my_mae.ipynb#ch0000027?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2089\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/training.py?line=2085'>2086</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/training.py?line=2086'>2087</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/training.py?line=2087'>2088</a>\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/training.py?line=2088'>2089</a>\u001b[0m   iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49msingle_batch_iterator(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistribute_strategy, x,\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/training.py?line=2089'>2090</a>\u001b[0m                                                 y, sample_weight,\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/training.py?line=2090'>2091</a>\u001b[0m                                                 class_weight)\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/training.py?line=2091'>2092</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/training.py?line=2092'>2093</a>\u001b[0m   logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1636\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[1;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1632'>1633</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1633'>1634</a>\u001b[0m   data \u001b[39m=\u001b[39m (x, y, sample_weight)\n\u001b[1;32m-> <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1635'>1636</a>\u001b[0m _check_data_cardinality(data)\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1636'>1637</a>\u001b[0m dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensors(data)\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1637'>1638</a>\u001b[0m \u001b[39mif\u001b[39;00m class_weight:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1645\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1643'>1644</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_data_cardinality\u001b[39m(data):\n\u001b[1;32m-> <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1644'>1645</a>\u001b[0m   num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m(\u001b[39mint\u001b[39;49m(i\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mflatten(data))\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1645'>1646</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(num_samples) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1646'>1647</a>\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mData cardinality is ambiguous:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1645\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1643'>1644</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_data_cardinality\u001b[39m(data):\n\u001b[1;32m-> <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1644'>1645</a>\u001b[0m   num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mint\u001b[39m(i\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(data))\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1645'>1646</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(num_samples) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///~/miniconda3/envs/tf/lib/site-packages/keras/engine/data_adapter.py?line=1646'>1647</a>\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mData cardinality is ambiguous:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# # single bacth\n",
    "X, y = GEN.next()\n",
    "X = X.astype(np.uint8)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "# mae_model.train_on_batch(X[0],y[0])\n",
    "history = mae_model.fit(\n",
    "    train_ds, epochs=EPOCHS, validation_data=train_ds, callbacks=train_callbacks,\n",
    ")\n",
    "\n",
    "# # mae_model = MaskedAutoencoder(\n",
    "# #     train_augmentation_model=train_augmentation_model,\n",
    "# #     test_augmentation_model=test_augmentation_model,\n",
    "# #     patch_layer=patch_layer,\n",
    "# #     patch_encoder=patch_encoder,\n",
    "# #     encoder=encoder,\n",
    "# #     decoder=decoder,\n",
    "# # )\n",
    "\n",
    "# # optimizer = tfa.optimizers.AdamW(learning_rate=scheduled_lrs, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# # # Compile and pretrain the model.\n",
    "# # mae_model.compile(\n",
    "# #     optimizer=optimizer, loss=keras.losses.MeanSquaredError(), metrics=[\"mae\"]\n",
    "# # )\n",
    "\n",
    "# # x = train_augmentation_model(X)\n",
    "# # x = patch_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, (X, y) in tqdm(enumerate(GEN), total=len(GEN)-1):\n",
    "    mae_model.train_on_batch(X, y)\n",
    "    # y_pred += CLASSIFIER.predict(X).tolist() \n",
    "    # y_true += y.tolist()\n",
    "    if b >= (GEN.samples / GEN.batch_size) - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mae_model.fit(GEN,\n",
    "                        epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8cbf28d3cb776c9509c084be644b2d084d902a2197dac10de5c316b508683de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ttds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
